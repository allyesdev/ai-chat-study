from typing import Literal, Optional, List, Tuple
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import (
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import os

Provider = Literal["openai", "ollama"]

# ì„¸ì…˜ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
STORE = {}

TEMPERATURE = 0.3
TOP_P = 0.9
MAX_TOKENS = 8000

# RAG ê´€ë ¨ ì„¤ì •
PDF_PATH = "data/tax_faq.pdf"
VECTOR_STORE_PATH = "data/vector_store"
SIMILARITY_THRESHOLD = 0.6  # ìœ ì‚¬ë„ ì„ê³„ê°’


def get_session_history(session_id: str):
    if session_id not in STORE:
        message_history = ChatMessageHistory()
        message_history.add_message(
            SystemMessage(
                content="You are a helpful assistant. Answer in Korean by default."
            )
        )
        STORE[session_id] = message_history
    return STORE[session_id]


def make_llm(provider: Provider = "openai", model: Optional[str] = None):
    if provider == "openai":
        return ChatOpenAI(
            model=model or "gpt-4.1-mini",
            temperature=TEMPERATURE,
            top_p=TOP_P,
        )
    else:
        return ChatOllama(
            model=model or "llama3.2",
            temperature=TEMPERATURE,
            top_p=TOP_P,
        )


def make_message_trimmer(llm):
    return trim_messages(
        max_tokens=MAX_TOKENS,
        strategy="last",
        token_counter=llm,
        include_system=True,
        start_on="human",
    )


def load_pdf_and_create_vectorstore():
    """PDFë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if os.path.exists(VECTOR_STORE_PATH):
        print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        return FAISS.load_local(
            VECTOR_STORE_PATH, OpenAIEmbeddings(), allow_dangerous_deserialization=True
        )

    print("PDFë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()

    # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
    from langchain.text_splitter import RecursiveCharacterTextSplitter

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)

    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())

    # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)
    vectorstore.save_local(VECTOR_STORE_PATH)

    return vectorstore


def analyze_intent(user_input: str, llm) -> bool:
    """ì‚¬ìš©ì ì…ë ¥ì´ ì—°ë§ì •ì‚° ê´€ë ¨ì¸ì§€ ë¶„ì„í•©ë‹ˆë‹¤."""
    intent_prompt = f"""
    ë‹¤ìŒ ì§ˆë¬¸ì´ ì—°ë§ì •ì‚°, ì„¸ê¸ˆ, ì†Œë“ì„¸, ê³µì œ, ì‹ ê³ , ë‚©ë¶€, ì„¸ë¬´ ê´€ë ¨ ë‚´ìš©ì¸ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.
    
    ì§ˆë¬¸: {user_input}
    
    ì—°ë§ì •ì‚° ê´€ë ¨ í‚¤ì›Œë“œ: ì—°ë§ì •ì‚°, ì†Œë“ì„¸, ì„¸ê¸ˆ, ê³µì œ, ì‹ ê³ , ë‚©ë¶€, ì„¸ë¬´, ê·¼ë¡œì†Œë“, ì‚¬ì—…ì†Œë“, ê¸°íƒ€ì†Œë“, 
    ì˜ë£Œë¹„ê³µì œ, êµìœ¡ë¹„ê³µì œ, ê¸°ë¶€ê¸ˆê³µì œ, ì—°ê¸ˆë³´í—˜ë£Œê³µì œ, ì£¼íƒìê¸ˆê³µì œ, ìë…€ì„¸ì•¡ê³µì œ, 
    êµ­ì„¸ì²­, í™ˆíƒìŠ¤, ì¢…í•©ì†Œë“ì„¸, ë¶€ê°€ê°€ì¹˜ì„¸, ì„¸ë²•, ì„¸ìœ¨, ê³¼ì„¸í‘œì¤€, ì„¸ì•¡ê³µì œ, ì„¸ì•¡ê°ë©´
    
    ì—°ë§ì •ì‚° ê´€ë ¨ì´ë©´ "YES", ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ "NO"ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    """

    response = llm.invoke([HumanMessage(content=intent_prompt)])
    return "YES" in response.content.upper()


def search_rag_documents(
    user_input: str, vectorstore, llm
) -> Tuple[List[Document], float]:
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(user_input)

    # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° - similarity_search_with_score ì‚¬ìš©
    try:
        docs_with_scores = vectorstore.similarity_search_with_score(user_input, k=3)
        if docs_with_scores:
            similarity_score = max(0, 1 - docs_with_scores[0][1])
        else:
            similarity_score = 0.0
    except:
        similarity_score = 0.8 if docs else 0.0

    return docs, similarity_score


def generate_rag_response(user_input: str, docs: List[Document], llm) -> str:
    context = "\n\n".join([doc.page_content for doc in docs])

    rag_prompt = f"""
    ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ì°¸ê³  ë¬¸ì„œ:
    {context}
    
    ì‚¬ìš©ì ì§ˆë¬¸: {user_input}
    
    ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
    - ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
    - ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
    - í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
    - êµ¬ì²´ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
    """

    response = llm.invoke([SystemMessage(content=rag_prompt)])
    return response.content


def make_chat_chain(llm):
    """
    trim_messagesë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ì œí•œí•˜ëŠ” ì±„íŒ… ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    trimmer = make_message_trimmer(llm)
    base = trimmer | llm

    return RunnableWithMessageHistory(base, get_session_history)


def process_user_query(user_input: str, llm, vectorstore, chat, cfg):
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë¡œì§"""
    print("ğŸ” ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘...")

    # 1. ì˜ë„ ë¶„ì„
    is_tax_related = analyze_intent(user_input, llm)

    if not is_tax_related:
        print("ğŸ’¬ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        print("ğŸ¤– AI: ", end="", flush=True)
        response = chat.invoke([HumanMessage(content=user_input)], cfg)
        print(response.content)
        return

    print("ğŸ“š ì—°ë§ì •ì‚° ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì¸ì‹í•˜ì—¬ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")

    # 2. RAG ê²€ìƒ‰
    docs, similarity_score = search_rag_documents(user_input, vectorstore, llm)
    print(f"ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}, ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score:.2f}")

    # 3. ìœ ì‚¬ë„ ê²€ì¦
    if similarity_score < SIMILARITY_THRESHOLD or not docs:
        print(
            "ğŸ¤– AI: ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ìë£Œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ êµ­ì„¸ì²­ í™ˆíƒìŠ¤(https://hometax.go.kr)ì—ì„œ í™•ì¸í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤."
        )
        return

    # 4. RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
    print("ğŸ¤– AI: ", end="", flush=True)
    rag_response = generate_rag_response(user_input, docs, llm)
    print(rag_response)


def main():
    provider = input("Provider ì„ íƒ (openai / ollama) [openai]: ").strip() or "openai"
    model = input("ëª¨ë¸ ì´ë¦„ (ë¹„ì›Œë‘ë©´ ê¸°ë³¸ê°’): ").strip() or None

    llm = make_llm(provider=provider, model=model)
    chat = make_chat_chain(llm)
    cfg = {"configurable": {"session_id": "multi-chat-session"}}

    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("ğŸš€ ì—°ë§ì •ì‚° RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    try:
        vectorstore = load_pdf_and_create_vectorstore()
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("ì¼ë°˜ ì±„íŒ… ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤...")
        vectorstore = None

    print("\n\nğŸ’¬ ì—°ë§ì •ì‚° ê°„ì†Œí™” ì§ˆë‹µ ì‹œìŠ¤í…œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    print("ì—°ë§ì •ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ì‹œë©´ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.")
    print("(ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit')")

    while True:
        user_input = input("\nğŸ‘¤ You: ")
        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if vectorstore:
            process_user_query(user_input, llm, vectorstore, chat, cfg)
        else:
            print("ğŸ¤– AI: ", end="", flush=True)
            response = chat.invoke([HumanMessage(content=user_input)], cfg)
            print(response.content)


if __name__ == "__main__":
    main()
