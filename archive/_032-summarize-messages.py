from typing import Literal, Optional, List
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

Provider = Literal["openai", "ollama"]

# ì„¸ì…˜ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
STORE = {}

TEMPERATURE = 0.3
TOP_P = 0.9
MAX_MESSAGES_TO_KEEP = 2  # ìµœê·¼ 2ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€


def get_session_history(session_id: str):
    if session_id not in STORE:
        STORE[session_id] = ChatMessageHistory()
    return STORE[session_id]


def make_llm(provider: Provider = "ollama", model: Optional[str] = None):
    if provider == "openai":
        return ChatOpenAI(
            model=model or "gpt-4o-mini",
            temperature=TEMPERATURE,
            top_p=TOP_P,
            streaming=True,
        )
    else:
        return ChatOllama(
            model=model or "llama3.2",
            temperature=TEMPERATURE,
            top_p=TOP_P,
            streaming=True,
        )


def summarize_messages(messages: List, llm) -> str:
    """
    ë©”ì‹œì§€ë“¤ì„ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    if not messages:
        return ""

    # ë©”ì‹œì§€ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    conversation_text = ""
    for msg in messages:
        if isinstance(msg, HumanMessage):
            conversation_text += f"ì‚¬ìš©ì: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            conversation_text += f"AI: {msg.content}\n"

    # ìš”ì•½ í”„ë¡¬í”„íŠ¸
    summarize_prompt = f"""
ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš©ë§Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:
{conversation_text}

ìš”ì•½:
"""

    try:
        response = llm.invoke(summarize_prompt)
        return response.content if hasattr(response, "content") else str(response)
    except Exception as e:
        print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


def summarize_messages(messages, llm):
    """
    ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ì •ë¦¬í•˜ê³  ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½í•©ë‹ˆë‹¤.
    """

    if len(messages) <= MAX_MESSAGES_TO_KEEP + 1:
        return

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ìµœê·¼ 2ê°œ ë©”ì‹œì§€ ë¶„ë¦¬
    system_messages = [msg for msg in messages if isinstance(msg, SystemMessage)]
    recent_messages = messages[-MAX_MESSAGES_TO_KEEP:]
    old_messages = messages[:-MAX_MESSAGES_TO_KEEP]  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸

    # ì˜¤ë˜ëœ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ìš”ì•½
    if old_messages:
        summary = summarize_messages(old_messages, llm)
        summary_message = SystemMessage(content=f"[ì´ì „ ëŒ€í™” ìš”ì•½] {summary}")

        # íˆìŠ¤í† ë¦¬ ì¬êµ¬ì„±
        new_messages = system_messages + [summary_message] + recent_messages
        messages.clear()
        for msg in new_messages:
            messages.append(msg)


def make_chat_chain(llm):
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "You are a helpful assistant. Answer in Korean by default."),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
        ]
    )
    base = prompt | llm

    return RunnableWithMessageHistory(
        base,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )


def stream_reply(*, chat, user_input, cfg, llm):
    """
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    # í˜„ì¬ ì„¸ì…˜ì˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    session_id = cfg["configurable"]["session_id"]
    history = get_session_history(session_id)

    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì •ë¦¬ ë° ìš”ì•½
    summarize_messages(history.messages, llm)

    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
    for chunk in chat.stream({"input": user_input}, cfg):
        text = getattr(chunk, "content", None)
        if text is None and isinstance(chunk, dict):
            text = chunk.get("output_text") or chunk.get("content") or ""
        if text is None:
            text = str(chunk)
        print(text, end="", flush=True)
    print("\n")


def main():
    provider = input("Provider ì„ íƒ (openai / ollama) [ollama]: ").strip() or "ollama"
    model = input("ëª¨ë¸ ì´ë¦„ (ë¹„ì›Œë‘ë©´ ê¸°ë³¸ê°’): ").strip() or None

    llm = make_llm(provider=provider, model=model)
    chat = make_chat_chain(llm)
    cfg = {"configurable": {"session_id": "summarize-chat-session"}}

    print("\nëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit')")
    print("ğŸ’¡ ìµœê·¼ 2ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€í•˜ê³  ì´ì „ ë©”ì‹œì§€ë“¤ì€ ìë™ìœ¼ë¡œ ìš”ì•½ë©ë‹ˆë‹¤.")

    while True:
        user_input = input("ğŸ‘¤ You: ")
        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        print("ğŸ¤– AI: ", end="", flush=True)
        stream_reply(chat=chat, user_input=user_input, cfg=cfg, llm=llm)


if __name__ == "__main__":
    main()
