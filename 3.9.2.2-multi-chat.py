from typing import Literal, Optional
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

Provider = Literal["openai", "ollama"]

# ì„¸ì…˜ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
STORE = {}

TEMPERATURE = 0.3
TOP_P = 0.9


def get_session_history(session_id: str):
    if session_id not in STORE:
        STORE[session_id] = ChatMessageHistory()
    return STORE[session_id]


def make_llm(provider: Provider = "ollama", model: Optional[str] = None):
    if provider == "openai":
        return ChatOpenAI(
            model=model or "gpt-4.1-mini",
            temperature=TEMPERATURE,
            top_p=TOP_P,
        )
    else:
        return ChatOllama(
            model=model or "llama3.2",
            temperature=TEMPERATURE,
            top_p=TOP_P,
        )


def make_chat_chain(llm):
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "You are a helpful assistant. Answer in Korean by default."),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
        ]
    )

    base = prompt | llm

    return RunnableWithMessageHistory(
        base,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: ì‚¬ìš©ìì™€ì˜ ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    ì‹¤í–‰ ê³¼ì •:
    1. ì‚¬ìš©ìë¡œë¶€í„° AI ì œê³µì—…ì²´ì™€ ëª¨ë¸ ì„ íƒ
    2. ì„ íƒëœ ì„¤ì •ìœ¼ë¡œ LLMê³¼ ì±„íŒ… ì²´ì¸ ì´ˆê¸°í™”
    3. ëŒ€í™” ë£¨í”„ ì‹¤í–‰ (ì‚¬ìš©ìê°€ ì¢…ë£Œí•  ë•Œê¹Œì§€)
    4. ê° ëŒ€í™”ëŠ” ì„¸ì…˜ ê¸°ë¡ì— ì €ì¥ë˜ì–´ ë§¥ë½ ìœ ì§€
    """
    provider = input("Provider ì„ íƒ (openai / ollama) [ollama]: ").strip() or "ollama"
    model = input("ëª¨ë¸ ì´ë¦„ (ë¹„ì›Œë‘ë©´ ê¸°ë³¸ê°’): ").strip() or None

    llm = make_llm(provider=provider, model=model)
    chat = make_chat_chain(llm)
    cfg = {"configurable": {"session_id": "multi-chat-session"}}

    print("\nëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit')")

    while True:
        user_input = input("ğŸ‘¤ You: ")

        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        res = chat.invoke({"input": user_input}, cfg)
        print(f"ğŸ¤– AI: {res.content}\n")


if __name__ == "__main__":
    main()
