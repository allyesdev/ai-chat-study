from typing import Literal, Optional
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import (
    HumanMessage,
    SystemMessage,
    trim_messages,
)

Provider = Literal["openai", "ollama"]

# ì„¸ì…˜ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
STORE = {}

TEMPERATURE = 0.3
TOP_P = 0.9
MAX_TOKENS = 45


def get_session_history(session_id: str):
    if session_id not in STORE:
        message_history = ChatMessageHistory()
        message_history.add_message(
            SystemMessage(
                content="You are a helpful assistant. Answer in Korean by default."
            )
        )
        STORE[session_id] = message_history
    return STORE[session_id]


def make_llm(provider: Provider = "ollama", model: Optional[str] = None):
    if provider == "openai":
        return ChatOpenAI(
            model=model or "gpt-4.1-mini",
            temperature=TEMPERATURE,
            top_p=TOP_P,
            streaming=True,  # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™œì„±í™” - ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì•„ì˜´
        )
    else:
        return ChatOllama(
            model=model or "llama3.2",
            temperature=TEMPERATURE,
            top_p=TOP_P,
            streaming=True,  # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™œì„±í™” - ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì•„ì˜´
        )


def make_message_trimmer(llm):
    return trim_messages(
        max_tokens=MAX_TOKENS,
        strategy="last",
        token_counter=llm,
        include_system=True,
        start_on="human",
    )


def make_chat_chain(llm):
    """
    trim_messagesë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ì œí•œí•˜ëŠ” ì±„íŒ… ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    trimmer = make_message_trimmer(llm)
    base = trimmer | llm

    return RunnableWithMessageHistory(base, get_session_history)


def stream_reply(*, chat, user_input, cfg):
    for chunk in chat.stream([HumanMessage(content=user_input)], cfg):
        text = getattr(chunk, "content", None)
        if text is None and isinstance(chunk, dict):
            text = chunk.get("output_text") or chunk.get("content") or ""
        if text is None:
            text = str(chunk)
        print(text, end="", flush=True)
    print("\n")


def main():
    provider = input("Provider ì„ íƒ (openai / ollama) [ollama]: ").strip() or "ollama"
    model = input("ëª¨ë¸ ì´ë¦„ (ë¹„ì›Œë‘ë©´ ê¸°ë³¸ê°’): ").strip() or None

    llm = make_llm(provider=provider, model=model)
    chat = make_chat_chain(llm)
    cfg = {"configurable": {"session_id": "multi-chat-session"}}

    print("\nëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit')")
    while True:
        user_input = input("ğŸ‘¤ You: ")
        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        print("ğŸ¤– AI: ", end="", flush=True)
        stream_reply(chat=chat, user_input=user_input, cfg=cfg)


if __name__ == "__main__":
    main()
